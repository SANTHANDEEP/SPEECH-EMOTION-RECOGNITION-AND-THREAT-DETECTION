# -*- coding: utf-8 -*-
"""Speech_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XgCgArJfBsTlLFmbqDKT_W_xChwewqYb
"""

import pandas as pad
import numpy as np
import glob
import soundfile
import os
import sys
import librosa.display as ld
import librosa
import librosa.display
import seaborn as sbn
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

from IPython.display import Audio
import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings ("ignore", category=DeprecationWarning)
RavdessData = "/content/drive/MyDrive/NLP/Audio_Speech_Actors_01-24"

import os
import pandas as pd  # Make sure you import the correct module name (pandas, not pad)

RavdessData = '/content/drive/MyDrive/NLP_Project/Audio_Speech_Actors_01-24'
ravdessDirectoryList = os.listdir(RavdessData)
fileEmotion = []
filePath = []

for dir in ravdessDirectoryList:
    actor_dir = os.path.join(RavdessData, dir)  # Use os.path.join to construct the correct path
    if os.path.isdir(actor_dir):  # Check if the directory exists
        actor = os.listdir(actor_dir)
        for file in actor:
            part = file.split('.')[0]
            part = part.split('-')
            fileEmotion.append(int(part[2]))
            filePath.append(os.path.join(actor_dir, file))  # Use os.path.join to construct the correct path
    else:
        print(f"Directory {actor_dir} does not exist.")

emotion_df = pd.DataFrame(fileEmotion, columns=['Emotions'])
path_df = pd.DataFrame(filePath, columns=['Path'])
Ravdess_df = pd.concat([emotion_df, path_df], axis=1)

Ravdess_df.Emotions.replace({1: 'neutral', 2: 'calm', 3:'happy', 4:'sad', 5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'}, inplace = True)
Ravdess_df.head()

dataPath = pad.concat([Ravdess_df], axis = 0)
dataPath.to_csv("data_path.csv", index=False)
dataPath.head()

import seaborn as sns
sns.set_style('whitegrid')
plt.figure(figsize=(8, 6))
plt.title('Count of Emotions', size=16)
emotion_counts = dataPath['Emotions'].value_counts()
emotions = emotion_counts.index
counts = emotion_counts.values
num_colors = len(emotions)
colors = sns.color_palette("husl", num_colors)
bars = plt.bar(emotions, counts, color=colors)
plt.ylabel('Count', size=12)
plt.xlabel('Emotions', size=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(np.arange(0, max(counts) + 1, step=max(counts) // 10))
sns.despine(top=True, right=True, left=False, bottom=False)
plt.show()

def createWaveplot (data, sr, e):
    plt.figure(figsize=(10, 3))
    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)
    librosa.display.waveplot(data, sr=sr)
    plt.show()

def createSpectrogram(data, sr, e):
    X = librosa.stft (data)
    Xdb = librosa.amplitude_to_db(abs(X))
    plt.figure (figsize=(12, 3))
    plt.title('Spectrogram for audio with {} emption'.format(e), size=15)
    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
    plt.colorbar()

"""Audio, Waveplot and Spectogram for different emotions"""

import numpy as np
import librosa
import matplotlib.pyplot as plt
from IPython.display import Audio

# Assuming you have a DataFrame called dataPath containing paths to audio files and their associated emotions

emotion = 'happy'
path = np.array(dataPath.Path[dataPath.Emotions==emotion])[1]
data, samplingRate = librosa.load(path)

# Create waveplot
plt.figure(figsize=(14, 5))
plt.plot(data)
plt.title('Waveplot for %s' % emotion)

# Create spectrogram
plt.figure(figsize=(14, 5))
plt.specgram(data, Fs=samplingRate)
plt.colorbar(format='%+2.0f dB')
plt.title('Spectrogram for %s' % emotion)

# Play audio
Audio(data, rate=samplingRate)

emotion = 'fear'
path = np.array(dataPath.Path[dataPath.Emotions == emotion])[1]
data, samplingRate = librosa.load(path)

# Create waveplot
plt.figure(figsize=(14, 5))
plt.plot(data)
plt.title('Waveplot for %s' % emotion)

# Create spectrogram
plt.figure(figsize=(14, 5))
spec = np.abs(librosa.stft(data))
spec = librosa.amplitude_to_db(spec, ref=np.max)
plt.imshow(spec, aspect='auto', origin='lower', extent=[0, len(data)/samplingRate, 0, samplingRate/2])
plt.colorbar(format='%+2.0f dB')
plt.title('Spectrogram for %s' % emotion)

# Play audio
Audio(data, rate=samplingRate)

import numpy as np
import librosa
import matplotlib.pyplot as plt
from IPython.display import Audio

# Assuming you have a DataFrame called dataPath containing paths to audio files and their associated emotions

emotion = 'angry'
path = np.array(dataPath.Path[dataPath.Emotions == emotion])[1]
data, samplingRate = librosa.load(path)

# Create waveplot
plt.figure(figsize=(14, 5))
plt.plot(data)
plt.title('Waveplot for %s' % emotion)

# Create spectrogram
plt.figure(figsize=(14, 5))
spec = np.abs(librosa.stft(data))
spec = librosa.amplitude_to_db(spec, ref=np.max)
plt.imshow(spec, aspect='auto', origin='lower', extent=[0, len(data)/samplingRate, 0, samplingRate/2])
plt.colorbar(format='%+2.0f dB')
plt.title('Spectrogram for %s' % emotion)

# Play audio
Audio(data, rate=samplingRate)

"""Applying various transformations to Audio data"""

from scipy.signal import resample
def noise (data):
    noiseAmp = 0.035* np.random.uniform()*np.amax (data)
    data = data + noiseAmp*np.random.normal (size=data.shape [0])
    return data

def stretch(data, factor=1.2):
    # Stretch the data using resampling
    resampled = resample(data, int(len(data)*factor))
    return resampled

def shift (data):
    shiftRange = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll (data, shiftRange)

def pitch(data, samplingRate, pitchFactor=0.7):
    return librosa.effects.pitch_shift(data, samplingRate, pitchFactor)

# Apply noise to the data
x = noise(data)

# Create waveform plot
plt.figure(figsize=(14, 4))
plt.plot(data)
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.show()

# Play audio with noise
Audio(x, rate=samplingRate)

x = stretch(data)

# Create waveform plot
plt.figure(figsize=(14, 4))
plt.plot(data)
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.show()

# Play stretched audio
Audio(x, rate=samplingRate)

x = shift(data)

# Create waveform plot
plt.figure(figsize=(14, 4))
plt.plot(data)
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.show()

# Play audio with noise
Audio(x, rate=samplingRate)

y = x
# Shift the pitch of the audio signal by 6 semitones
y_shifted = librosa.effects.pitch_shift(y, sr=samplingRate, n_steps=6, bins_per_octave=12)

# Create waveform plot
plt.figure(figsize=(14, 4))
plt.plot(y_shifted)
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.show()

# Play the shifted audio
Audio(y_shifted, rate=samplingRate)

import librosa
import soundfile
import os, glob, pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

def pad_audio(data, length=2048):
    if len(data) < length:
        pad_width = length - len(data)
        data = np.pad(data, (0, pad_width), mode='constant')
    return data

import os
root = "/content/drive/MyDrive/NLP_Project/Audio_Speech_Actors_01-24"
os.chdir(root)

ls

""" "extract_feature" which is designed to extract various audio features from a sound file"""

def extract_feature(file_name, mfcc=True, chroma=True, mel=True):
    try:
        with soundfile.SoundFile(file_name) as sound_file:
            X = sound_file.read(dtype="float32")
            sample_rate = sound_file.samplerate

            # Example: Pad short audio files to a consistent length
            X = pad_audio(X)

            features = []

            # Compute STFT
            stft = np.abs(librosa.stft(X))

            if mfcc:
                mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
                features.append(mfccs)
            if chroma:
                chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
                features.append(chroma)
            if mel:
                mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)
                features.append(mel)

            # Check if any features were extracted
            if len(features) == 0:
                return None

            # Stack features into a single array
            result = np.hstack(features)

    except Exception as e:
        print(f"Error extracting features from {file_name}: {str(e)}")
        return None

    return result

"""mapping between emotion codes and their corresponding labels"""

# Emotions in the RAVDESS dataset
emotions={
  '01':'neutral',
  '02':'calm',
  '03':'happy',
  '04':'sad',
  '05':'angry',
  '06':'fearful',
  '07':'disgust',
  '08':'surprised'
}

#Emotions to observe
observed_emotions=['calm', 'happy', 'fearful', 'disgust']

def load_data(test_size=0.2):
    x, y = [], []
    expected_shape = None
    for file in glob.glob("/content/drive/MyDrive/NLP_Project/Audio_Speech_Actors_01-24/Actor_*/*.wav"):
        file_name = os.path.basename(file)
        emotion = emotions[file_name.split("-")[2]]
        if emotion not in observed_emotions:
            continue
        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)

        # Skip if the feature extraction returns an empty array
        if feature is None or len(feature) == 0:
            continue

        # Initialize expected_shape with the shape of the first feature
        if expected_shape is None:
            expected_shape = feature.shape

        # Ensure all features are of consistent dimensions
        if feature.shape != expected_shape:
            print(f"Inconsistent feature shape: {feature.shape} (expected {expected_shape})")
            continue

        x.append(feature)
        y.append(emotion)

    x = np.array(x)
    y = np.array(y)

    return train_test_split(x, y, test_size=test_size, random_state=9)

#Split the dataset
x_train,x_test,y_train,y_test=load_data(test_size=0.25)

x_train

#Get the shape of the training and testing datasets
print((x_train.shape[0], x_test.shape[0]))

#Get the number of features extracted
print(f'Features extracted: {x_train.shape[1]}')

#Initialize the Multi Layer Perceptron Classifier
model=MLPClassifier(alpha=0.1, batch_size=300, epsilon=1e-08, hidden_layer_sizes=(600,), learning_rate='adaptive', max_iter=500)

model.fit(x_train,y_train)

#Predict for the test set
expected_y = y_test
y_pred=model.predict(x_test)

y_pred

print(metrics.confusion_matrix(expected_y, y_pred))

print(classification_report(y_test, y_pred))

#Calculate the accuracy of our model
accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)
#Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy*100))

from sklearn.metrics import accuracy_score, f1_score

f1_score(y_test, y_pred,average=None)

import pandas as pd
df=pd.DataFrame({'Actual': y_test, 'Predicted':y_pred})
df.head(10)

import pickle
# Writing different model files to file
with open( 'modelForPrediction1.sav', 'wb') as f:
    pickle.dump(model,f)

filename = 'modelForPrediction1.sav'
loaded_model = pickle.load(open(filename, 'rb')) # loading the model file from the storage
feature=extract_feature("/content/drive/MyDrive/NLP_Project/audio_speech_actors_01-24/Actor_20/03-01-01-01-02-01-20.wav", mfcc=True, chroma=True, mel=True)
feature=feature.reshape(1,-1)
prediction=loaded_model.predict(feature)
prediction

"""Real Time Application"""

pip install SpeechRecognition

import speech_recognition as sr
import wave
# create an AudioFile object from the WAV file
audio_file = sr.AudioFile('/content/drive/MyDrive/NLP_Project/audio_speech_actors_01-24/Actor_02/03-01-06-01-01-01-02.wav')
# instantiate the speech recognizer
recognizer = sr.Recognizer()
# extract audio data from the file
with audio_file as source:
    audio_data = recognizer.record(source)
# use the recognizer to convert audio to text
text = recognizer.recognize_google(audio_data)
# split the text into words
words = text.split()
# create a dictionary with the words and their counts
word_counts = {}
for word in words:
    if word in word_counts:
        word_counts[word] += 1
    else:
        word_counts[word] = 1
# print the dictionary
print(word_counts)

text = "This is a sample text to test if the script will kill or destroy the mood."

# List of threatening words
threatening_words = ['kill', 'murder', 'destroy', 'attack']

# Check if the text contains any threatening words
if any(word in text.lower() for word in threatening_words):
    print('Error: Text contains threatening words.')
else:
    # Split the text into words
    words = text.split()

    # Create a dictionary with the words and their counts
    word_counts = {}
    for word in words:
        # Convert word to lowercase to ensure case-insensitive counting
        word = word.lower()
        # Remove punctuation from the word
        word = ''.join(char for char in word if char.isalnum())
        if word in word_counts:
            word_counts[word] += 1
        else:
            word_counts[word] = 1

    # Print the dictionary
    print(word_counts)

# Calculate the accuracy of our model
accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)

# Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy * 100))

# Import required libraries for plotting
import matplotlib.pyplot as plt

# Create a figure and axis
fig, ax = plt.subplots(figsize=(6, 4))

# Create a bar chart
ax.bar(['Accuracy'], [accuracy], color='blue')

# Set the y-axis limit
ax.set_ylim(0, 1)

# Set the title and axis labels
ax.set_title('Model Accuracy', fontsize=14)
ax.set_xlabel('Metric', fontsize=12)
ax.set_ylabel('Score', fontsize=12)

# Add value labels on top of the bars
ax.bar_label(ax.containers[0], labels=[f'{accuracy * 100:.2f}%'])

# Adjust the spacing between subplots
plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.15)

# Show the plot
plt.show()

from sklearn.metrics import f1_score
import numpy as np
import matplotlib.pyplot as plt

# Calculate F1-score for each class
f1_scores = f1_score(y_test, y_pred, average=None)

# Get the class labels
class_labels = np.unique(y_test)

# Create a figure and axis
fig, ax = plt.subplots(figsize=(10, 6))

# Create a bar chart
x = np.arange(len(class_labels))
ax.bar(x, f1_scores, color='green')

# Set the x-ticks and labels
ax.set_xticks(x)
ax.set_xticklabels(class_labels, rotation=45, ha='right')

# Set the y-axis limit
ax.set_ylim(0, 1)

# Set the title and axis labels
ax.set_title('F1-Score per Class', fontsize=16)
ax.set_xlabel('Class', fontsize=14)
ax.set_ylabel('F1-Score', fontsize=14)

# Add value labels on top of the bars
for i, score in enumerate(f1_scores):
    ax.text(i, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=12)

# Adjust the spacing between subplots
plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.2)

# Show the plot
plt.show()

from sklearn.metrics import recall_score
import numpy as np
import matplotlib.pyplot as plt
# Calculate recall for each class
recall_scores = recall_score(y_test, y_pred, average=None)
print("Recall Scores:")
for label, score in zip(class_labels, recall_scores):
    print(f"Class {label}: {score:.2f}")
print()

# Get the class labels
class_labels = np.unique(y_test)

# Create a figure and axis
fig, ax = plt.subplots(figsize=(10, 6))

# Create a bar chart
x = np.arange(len(class_labels))
ax.bar(x, recall_scores, color='orange')

# Set the x-ticks and labels
ax.set_xticks(x)
ax.set_xticklabels(class_labels, rotation=45, ha='right')

# Set the y-axis limit
ax.set_ylim(0, 1)

# Set the title and axis labels
ax.set_title('Recall Score per Class', fontsize=16)
ax.set_xlabel('Class', fontsize=14)
ax.set_ylabel('Recall Score', fontsize=14)

# Add value labels on top of the bars
for i, score in enumerate(recall_scores):
    ax.text(i, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=12)

# Adjust the spacing between subplots
plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.2)

# Show the plot
plt.show()

from sklearn.metrics import precision_score
import numpy as np
import matplotlib.pyplot as plt
precision_scores = precision_score(y_test, y_pred, average=None)
print("Precision Scores:")
for label, score in zip(class_labels, precision_scores):
    print(f"Class {label}: {score:.2f}")
print()

# Get the class labels
class_labels = np.unique(y_test)

# Print the precision values
print("Precision Scores:")
for label, score in zip(class_labels, precision_scores):
    print(f"Class {label}: {score:.2f}")
print()

# Create a figure and axis
fig, ax = plt.subplots(figsize=(10, 6))

# Create a bar chart
x = np.arange(len(class_labels))
ax.bar(x, precision_scores, color='purple')

# Set the x-ticks and labels
ax.set_xticks(x)
ax.set_xticklabels(class_labels, rotation=45, ha='right')

# Set the y-axis limit
ax.set_ylim(0, 1)

# Set the title and axis labels
ax.set_title('Precision Score per Class', fontsize=16)
ax.set_xlabel('Class', fontsize=14)
ax.set_ylabel('Precision Score', fontsize=14)

# Add value labels on top of the bars
for i, score in enumerate(precision_scores):
    ax.text(i, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=12)

# Adjust the spacing between subplots
plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.2)

# Show the plot
plt.show()

# prompt: comparison of recall,precision,f1score,accuracy

import matplotlib.pyplot as plt
# Calculate the accuracy, precision, recall, and F1-score
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1_score = f1_score(y_test, y_pred, average='weighted')

# Print the results
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1_score)

# Create a bar chart to compare the metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
values = [accuracy, precision, recall, f1_score]

plt.bar(metrics, values, color='lightblue')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.title('Comparison of Evaluation Metrics')
plt.show()

import speech_recognition as sr
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from textblob import TextBlob
from pydub import AudioSegment

# Function to convert MP3 to WAV
def convert_mp3_to_wav(mp3_file, wav_file):
    sound = AudioSegment.from_mp3(mp3_file)
    sound.export(wav_file, format="wav")

# Function to transcribe audio to text
def transcribe_audio(audio_file):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio_data = recognizer.record(source)
        text = recognizer.recognize_google(audio_data)
    return text

# Function to clean and preprocess text
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalpha()]
    return ' '.join(filtered_tokens)

# Function for emotion detection and intensity analysis
def analyze_emotion(text):
    tblob = TextBlob(text)
    sentiment = tblob.sentiment
    polarity = sentiment.polarity
    subjectivity = sentiment.subjectivity
    return polarity, subjectivity

# Example usage
mp3_file = '/content/drive/MyDrive/TensorGo Assignment/Audio_files_used/Iron_Man.mp3'
wav_file = 'converted_audio.wav'

# Convert MP3 to WAV
convert_mp3_to_wav(mp3_file, wav_file)

# Transcribe WAV file to text
transcribed_text = transcribe_audio(wav_file)

# Clean and preprocess text
cleaned_text = preprocess_text(transcribed_text)

# Analyze emotion and intensity
polarity, subjectivity = analyze_emotion(cleaned_text)

print(f"Transcribed Text: {transcribed_text}")
print(f"Cleaned Text: {cleaned_text}")
print(f"Emotion Polarity: {polarity}, Subjectivity: {subjectivity}")

if polarity < -0.3:
  print("Sadness or Anger")
elif polarity < 0.5:
  print("Neutral")
else:
  print("Happiness or Excitement")

if subjectivity < 0.5:
  print("Fact")
else:
  print("Personal Opinion")